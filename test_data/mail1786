From: Ian
Date: 9 June 2012 at 02:54
Message-ID: 1786
Subject: Closed captioning 
To: Abby
Content:
Closed captioning is the process of displaying text on a television, video screen or other visual display to provide additional or interpretive information to individuals who wish to access it. Closed captions typically show a transcription of the audio portion of a program as it occurs (either verbatim or in edited form), sometimes including non-speech elements. The term "closed" in closed captioning indicates that not all viewers see the captionsonly those who choose to decode or activate them. This distinguishes from "open captions" (sometimes called "burned-in" or "hardcoded" captions), which are visible to all viewers. Most of the world does not distinguish captions from subtitles. In the United States and Canada, these terms do have different meanings, however: "subtitles" assume the viewer can hear but cannot understand the language or accent, or the speech is not entirely clear, so they only transcribe dialogue and some on-screen text. "Captions" aim to describe to the deaf and hard of hearing all significant audio contentspoken dialogue and non-speech information such as the identity of speakers and, occasionally, their manner of speakingalong with music or sound effects using words or symbols. The United Kingdom, Ireland, and most other countries do not distinguish between subtitles and closed captions, and use "subtitles" as the general termthe equivalent of "captioning" is usually referred to as "Subtitles for the hard of hearing". Their presence is referenced on screen by notation which says "Subtitles", or previously "Subtitles 888" or just "888" (the latter two are in reference to the conventional teletext channel for captions). Closed captions were created for the deaf community or hard of hearing individuals to assist in comprehension. They can also be used as a tool by those learning to read, learning to speak a non-native language, or in an environment where the audio is difficult to hear or is intentionally muted. Captions can also be used by viewers who simply wish to read a transcript along with the program audio. In the United States, the National Captioning Institute noted that English as a foreign or second language (ESL) learners were the largest group buying decoders in the late 1980s and early 1990s before built-in decoders became a standard feature of US television sets. This suggested that the largest audience of closed captioning was people whose native language was not English. In the United Kingdom, of 7.5 million people using TV subtitles (closed captioning), 6 million have no hearing impairment.[1] Closed captions are also used in public environments, such as bars and restaurants, where patrons may not be able to hear over the background noise, or where multiple televisions are displaying different programs.[2][3][4] Some television sets can be set to automatically turn captioning on when the volume is muted. For live programs, spoken words comprising the television program's soundtrack are transcribed by a human operator (a speech-to-text reporter) using stenotype or stenomask type of machines, whose phonetic output is instantly translated into text by a computer and displayed on the screen. This technique was developed in the 1970s as an initiative of the BBC's Ceefax teletext service.[5] In collaboration with the BBC, a university student took on the research project of writing the first phonetics-to-text conversion program for this purpose. Sometimes, the captions of live broadcasts, like news bulletins, sports events, live entertainment shows, and other live shows fall behind by a few seconds. This delay is because the machine does not know what the person is going to say next, so after the person on the show says the sentence, the captions appear.[6] Automatic computer speech recognition now works well when trained to recognize a single voice, and so since 2003, the BBC does live subtitling by having someone re-speak what is being broadcast. Live captioning is also a form of real-time text. Meanwhile, sport events on channels like ESPN are using court reporters, using a special (steno) keyboard and individually constructed dictionaries. In some cases, the transcript is available beforehand and captions are simply displayed during the program after being edited. For programs that have a mix of pre-prepared and live content, such as news bulletins, a combination of the above techniques is used. For prerecorded programs, commercials, and home videos, audio is transcribed and captions are prepared, positioned, and timed in advance. For all types of NTSC programming, captions are "encoded" into line 21 of the vertical blanking interval  a part of the TV picture that sits just above the visible portion and is usually unseen. For ATSC (digital television) programming, three streams are encoded in the video: two are backward compatible "line 21" captions, and the third is a set of up to 63 additional caption streams encoded in EIA-708 format.[7] Captioning is transmitted and stored differently in PAL and SECAM countries, where teletext is used rather than line 21, but the methods of preparation are similar. For home videotapes, a variation of the line 21 system is used in PAL countries.[attribution needed] Teletext captions can't be stored on a standard VHS tape (due to limited bandwidth), although they are available on S-VHS tapes and DVDs. For older televisions, a set-top box or other decoder is usually required. In the US, since the passage of the Television Decoder Circuitry Act, manufacturers of most television receivers sold have been required to include closed captioning display capability. High-definition TV sets, receivers, and tuner cards are also covered, though the technical specifications are different (high-definition display screens, as opposed to high-definition TVs, may lack captioning.) Canada has no similar law, but receives the same sets as the US in most cases. There are two styles of line 21 closed captioning:[citation needed] Example: (Man) I GOT THE MACHINE READY. (engine starting) A single program may include scroll-up and pop-on captions (e.g., scroll-up for narration and pop-on for song lyrics). A musical note symbol (sharp sign in UK, Ireland and Australia) is used to indicate song lyrics or background music. Generally, lyrics are preceded and followed by music notes (or hash signs), while song titles are bracketed like a sound effect. Standards vary from country to country and company to company. For live programs, some soap operas, and other shows captioned using scroll-up, Line 21 caption text include the symbols '>>' to indicate a new speaker (the name of the new speaker sometimes appears as well), and '>>>' in news reports to identify a new story. In some cases, '>>' means one person is talking and '>>>' means two or more people are talking. Capitals are frequently used because many older home caption decoder fonts had no descenders for the lowercase letters g, j, p, q, and y, though virtually all modern TVs have caption character sets with descenders. Text can be italicized, among a few other style choices. Captions can be presented in different colors as well. Coloration is rarely used in North America, but can sometimes be seen on music videos on MTV or VH-1, and in the captioning's production credits. More often, coloration is used in the United Kingdom, Ireland, Australia and New Zealand for speaker differentiation. There were many shortcomings in the original Line 21 specification from a typographic standpoint, since, for example, it lacked many of the characters required for captioning in languages other than English. Since that time, the core Line 21 character set has been expanded to include quite a few more characters, handling most requirements for languages common in North and South America such as French, Spanish, and Portuguese, though those extended characters are not required in all decoders and are thus unreliable in everyday use. The problem has been almost eliminated with the EIA-708 standard for digital television, which boasts a far more comprehensive character set. Captions are often edited to make them easier to read and to reduce the amount of text displayed onscreen. This editing can be very minor, with only a few occasional unimportant missed lines, to severe, where virtually every line spoken by the actors is condensed. The measure used to guide this editing is words per minute, commonly varying from 180 to 300, depending on the type of program. Offensive words are also captioned, but if the program is censored for TV broadcast, the broadcaster might not have arranged for the captioning to be edited or censored also. The "TV Guardian", a television set top box, is available to parents who wish to censor offensive language of programsthe video signal is fed into the box and if it detects an offensive word in the captioning, the audio signal is bleeped or muted for that period of time. Software programs are now available that automatically generate a closed-captioning of conversations. Examples of such conversations include discussions in conference rooms, classroom lectures, and/or religious services.[8] The Line 21 data stream can consist of data from several data channels multiplexed together. Field 1 has four data channels: two Captions (CC1, CC2) and two Text (T1, T2). Field 2 has five additional data channels: two Captions (CC3, CC4), two Text (T3, T4), and Extended Data Services (XDS). XDS data structure is defined in CEA608. As CC1 and CC2 share bandwidth, if there is a lot of data in CC1, there will be little room for CC2 data. Similarly CC3 and CC4 share the second field of line 21. Since some early caption decoders supported only CC1 and CC2, captions in a second language were often placed in CC2. This led to bandwidth problems, however, and the current U.S. Federal Communications Commission (FCC) recommendation is that bilingual programming should have the second caption language in CC3. Telemundo, for example, provides English subtitles for many of its Spanish programs in CC3. NTSC DVDs may carry closed captions in data packets of the MPEG-2 video streams inside of the Video-TS folder. Once played out of the analog outputs of a set top DVD player, the caption data is converted to the Line 21 format.[9] They are sent to the TV by the player and can be displayed with a TV's built-in decoder or a set-top decoder as usual. When viewed on a personal computer, caption data can be viewed by software that can read and decode the caption data packets in the MPEG-2 streams of the DVD-Video disc. Windows Media Player (before Windows 7) in Vista supported only closed caption channels 1 and 2 (not 3 or 4). And Apple's DVD Player does not have the ability to read and decode Line 21 caption data which is recorded on a DVD made from an over-the-air broadcast. Apple's DVD Player can display some movie DVD captions. In addition to Line 21 closed captions, video DVDs may also carry subtitles as a bitmap overlay which can be turned on and off via a set top DVD player or DVD player software, just like captions. This type of captioning is usually carried in a subtitle track labeled either "English for the hearing impaired" or, more recently, "SDH" (Subtitled for the Deaf and Hard of hearing).[10] Many popular Hollywood DVD-Videos can carry both subtitles and closed captions (see Stepmom DVD by Columbia Pictures). On some DVDs, the Line 21 captions may contain the same text as the subtitles; on others, only the Line 21 captions include the additional non-speech information (even sometimes song lyrics) needed for deaf and hard of hearing viewers. European Region 2 DVDs do not carry Line 21 captions, and instead list the subtitle languages availableEnglish is often listed twice, one as the representation of the dialogue alone, and a second subtitle set which carries additional information for the deaf and hard of hearing audience. (Many deaf/HOH subtitle files on DVDs are reworkings of original teletext subtitle files.) HD DVD and Blu-ray disc media cannot carry Line 21 closed captioning due to the design of High-Definition Multimedia Interface (HDMI) specifications that were designed to replace older analog and digital standards, such as VGA, S-Video, and DVI. Both Blu-ray disc and HD DVD can use either PNG bitmap subtitles or 'advanced subtitles' to carry SDH type subtitling, the latter being an XML based textual format which includes font, styling and positioning information as well as a unicode representation of the text. Advanced subtitling can also include additional media accessibility features such as "descriptive audio". There are several competing technologies used to provide captioning for movies in theaters. Cinema captioning falls into the categories of 'open' and 'closed.' The definition of "closed" captioning in this context is different from television, as it refers to any technology that allows as few as one member of the audience to view the captions. Open captioning in a film theater can be accomplished through burned-in captions, projected text or bitmaps, or (rarely) a display located above or below the movie screen. Typically, this display is a large LED sign. In a digital theater, open caption display capability is built into the digital projector. Closed caption capability is also available, with the ability for 3rd party closed caption devices to plug into the digital cinema server. Probably the best-known closed captioning option for film theaters is the Rear Window Captioning System from the National Center for Accessible Media. Upon entering the theater, viewers requiring captions are given a panel of flat translucent glass or plastic on a gooseneck stalk, which can be mounted in front of the viewer's seat. In the back of the theater is an LED display that shows the captions in mirror image. The panel reflects captions for the viewer, but is nearly invisible to surrounding patrons. The panel can be positioned so that the viewer watches the movie through the panel and captions appear either on or near the movie image. A company called Cinematic Captioning Systems has a similar reflective system called Bounce Back. A major problem for distributors has been that these systems are each proprietary, and require separate distributions to the theater to enable them to work. Proprietary systems also incur license fees. For film projection systems, Digital Theater Systems, the company behind the DTS surround sound standard, has created a digital captioning device called the DTS-CSS or Cinema Subtitling System. It is a combination of a laser projector which places the captioning (words, sounds) anywhere on the screen and a thin playback device with a CD that holds many languages. If the Rear Window Captioning System is used, the DTS-CSS player is also required for sending caption text to the Rear Window sign located in the rear of the theater. Special effort has been made to build accessibility features into digital projection systems (see digital cinema). Through SMPTE, standards now exist that dictate how open and closed captions, as well as hearing-impaired and visually impaired narrative audio, are packaged with the rest of the digital movie. This eliminates the proprietary caption distributions required for film, and the associated royalties. SMPTE has also standardized the communication of closed caption content between the digital cinema server and 3rd party closed caption systems (the CSP/RPL protocol). As a result, new, competitive closed caption systems for digital cinema are now emerging that will work with any standards-compliant digital cinema server. These newer closed caption devices include cup-holder-mounted electronic displays and wireless glasses which display caption text in front of the wearer's eyes.[11] Bridge devices are also available to enable the use of Rear Window systems. As of mid-2010, the remaining challenge to the wide introduction of accessibility in digital cinema is the industry-wide transition to SMPTE DCP, the standardized packaging method for very high quality, secure distribution of digital movies. Closed captioning of video games is becoming more common. One of the first video games to feature true closed captioning was Zork Grand Inquisitor in 1997.[12] Many games since then have at least offered subtitles for spoken dialog during cut scenes, and many include significant in-game dialog and sound effects in the captions as well; for example, with subtitles turned on in the Metal Gear Solid series of stealth games, not only are subtitles available during cut scenes, but any dialog spoken during real-time gameplay will be captioned as well, allowing players who can't hear the dialog to know what enemy guards are saying and when the main character has been detected. Also, in many of developer Valve's video games (such as Half-Life 2 or Left 4 Dead), when closed captions are activated, dialog and nearly all sound effects either made by the player or from other sources (e.g. gunfire, explosions) will be captioned. Video games don't offer Line 21 captioning, decoded and displayed by the television itself but rather a built-in subtitle display, more akin to that of a DVD. The game systems themselves have no role in the captioning either: each game must have its subtitle display programmed individually. Reid Kimball, a game designer who is hearing impaired, is attempting to educate game developers about closed captioning for games. Reid started the Games[CC] group to closed caption games and serve as a research and development team to aid the industry. Kimball designed the Dynamic Closed Captioning system,[citation needed] writes articles, and speaks at developer conferences. Games[CC]'s first closed captioning project called Doom3[CC] was nominated for an award as Best Doom3 Mod of the Year for IGDA's Choice Awards 2006 show. Internet video streaming service YouTube offers captioning services in videos. The author of the video can upload a SubViewer (*.SUB), SubRip (*.SRT) or *.SBV file.[13] YouTube is currently testing an Automatic Caption Feature, which will transcribe audio and not require the author to add a captioning file. This feature was only available on certain videos, but now is available to all English videos.[14] However, the automatic captioning is often inaccurate on videos with background music and exaggerated emotion in speaking. On June 30, 2010, YouTube announced a new "YouTube Ready" designation for professional caption vendors in the United States.[15] The initial list included twelve companies who passed a caption quality evaluation administered by the Described and Captioned Media Project, have a website and a YouTube channel where customers can learn more about their services, and have agreed to post rates for the range of services that they offer for YouTube content. Flash video also supports captions via the Distribution Exchange profile (DFXP)of W3C Timed Text format. The latest Flash authoring software adds free player skins and caption components that enable viewers to turn captions on/off during playback from a webpage. Previous versions of Flash relied on the Captionate 3rd party component and skin to caption Flash video. Custom Flash players designed in Flex can be tailored to support the Timed Text exchange profile, Captionate .XML, or SAMI file (see Hulu captioning). The Silverlight Media Framework.[16] also includes support for the Timed Text exchange profile for both download and adaptive streaming media. Windows Media Video can support closed captions for both video on demand streaming or live streaming scenarios. Typically Windows Media captions support the SAMI file format but can also carry embedded closed caption data. QuickTime video supports true 608 caption data via QuickTime's proprietary Closed Caption Track. These captions can be turned on and off and appear in the same style as TV closed captions with all the standard formatting (pop-on, roll-up, paint-on) and can be positioned and split anywhere on the video screen. QuickTime Closed Caption tracks can be viewed in Mac or Windows versions of QuickTime Player, iTunes (via QuickTime), iPod Nano, iPod Classic, iPod Touch, iPhone, and iPad. Live plays can be open captioned by a captioner who displays lines from the script and including non-speech elements on a large display screen near the stage.[17] A captioned telephone (also called captioned relay or Cap-Tel) is a telephone that displays real-time captions of the current conversation. The captions are typically displayed on a screen embedded into the telephone base. In the United States especially, most media monitoring services capture and index closed captioning text from news and public affairs programs, allowing them to search the text for client references. The use of closed captioning for television news monitoring was pioneered by Universal Press Clipping Bureau (Universal Information Services) in 1992[citation needed], and later in 1993 by Tulsa-based NewsTrak of Oklahoma (later known as Broadcast News of Mid-America, acquired by video news release pioneer Medialink Worldwide Incorporated in 1997).[citation needed] US patent 7,009,657 describes a "method and system for the automatic collection and conditioning of closed caption text originating from multiple geographic locations" as used by news monitoring services. The US ATSC HDTV system originally specified two different kinds of closed captioning datastream standardsthe original (available by Line 21) and another more modern version encoded in MPEG-2, the CEA-708 standard.[7] The US FCC mandates that broadcasters deliver (and generate, if necessary) both datastream formats.[7] The Canadian CRTC has not mandated that broadcasters either broadcast both datastream formats or exclusively in one format. Many viewers find that when they switch to an HDTV they are unable to view closed caption (CC) information, even though the broadcaster is sending it and the TV is able to display it. Originally, CC information was included in the picture ("line 21"), but there is no equivalent capability in the HDTV 720p/1080i interconnects between the display and a "source". A "source", in this case, can be a DVD player or an HD tuner (a cable box is an HD tuner). When CC information is encoded in the MPEG-2 data stream, only the device that decodes the MPEG-2 data (a source) has access to the closed caption information; there is no standard for transmitting the CC information to an HD display separately. Thus, if there is CC information, the source device needs to overlay the CC information on the picture prior to transmitting to the display over the interconnect. Many source devices do not have the ability to overlay CC information, for controlling the CC overlay is extremely complicated. For example, the Motorola DCT-5xxx and -6xxx cable set-top boxes have the ability to decode CC information located on the mpg stream and overlay it on the picture, but turning CC on and off requires turning off the unit and going into a special setup menu (it is not on the standard configuration menu and it cannot be controlled using the remote). Historically, DVD players and cable box tuners did not need to do this overlaying, they simply passed this information on to the TV, and they are not mandated to perform this overlaying. Many modern HDTVs can be directly connected to cables, but then they often cannot receive scrambled channels that the user is paying for. Thus, the lack of a standard way of sending CC information between components, along with the lack of a mandate to add this information to a picture, results in CC being unavailable to many hard-of-hearing and deaf users. The European teletext systems are the source for closed captioning signals, thus when teletext is embedded into DVB-T or DVB-S the closed captioning signal is included.[18] However, for DVB-T and DVB-S, it is not necessary for a teletext page signal to also be present (ITV1, for example, does not carry analogue teletext signals on Sky Digital, but does carry the embedded version, accessible from the "Services" menu of the receiver, or more recently by turning them off/on from a mini menu accessible from the "help" button). The CEA-708 specification provides for dramatically improved captioning As of 2009, however, most closed captioning for DTV environments is done using tools designed for analog captioning (working to the CEA-608 NTSC spec rather than the CEA-708 DTV spec). The captions are then run through transcoders made by companies like EEG Enterprises or Evertz, which convert the analog Line 21 caption format to the digital format. This means that none of the CEA-708 features are used unless they were also contained in CEA-608. In April, 2010, Sony Creative Software released the Vegas Pro 9.0d update to the professional non-linear editor, Vegas Pro which implemented basic support for importing, editing, and delivering CEA608 Closed Captions. Vegas Pro 10, released on October 11, 2010, added several enhancements to the closed captioning support. TV-like CEA608 Closed Captioning can now be displayed as an overlay when played back in the Preview and Trimmer windows making it easy to check placement, edits, and timing of CC information. CEA708 style Closed Captioning is automatically created when the CEA608 data is created. Line 21 Closed Captioning is now supported as well as HD-SDI closed captioning capture and print from AJA and Blackmagic Design cards. Line 21 support provides a workflow for existing legacy media. Other improvements include increased support for multiple closed captioning file types, as well as the ability to export closed caption data for DVD Architect, YouTube, RealPlayer, QuickTime, and Windows Media Player. In mid 2009, Apple released Final Cut Pro version 7 and began support for inserting closed caption data into SD and HD tape masters via firewire and compatible video capture cards.[19] Up until this time it was not possible for video editors to insert caption data with both CEA-608 and CEA-708 to their tape masters. The typical workflow included first printing the SD or HD video to a tape and sending it to a professional closed caption service company that had a stand alone closed caption hardware encoder. This new closed captioning workflow known as e-Captioning involves making a proxy video from the non-linear system to import into a third-party non-linear closed captioning software. Once the closed captioning software project is completed, it must export a closed caption file compatible with the non-linear editing system. In the case of Final Cut Pro 7, three different file formats can be accepted: a .SCC file (Scenarist Closed Caption file) for Standard Definition video, a QuickTime 608 Closed Caption track (a special 608 coded track in the .mov file wrapper) for Standard Definition video, and finally a QuickTime 708 Closed Caption track (a special 708 coded track in the .mov file wrapper) for High Definition video output. Alternatively, Matrox video systems devised another mechanism for inserting closed caption data by allowing the video editor to include CEA-608 and CEA-708 in a discrete audio channel on the video editing timeline. This allows real-time preview of the captions while editing and is compatible with Final Cut Pro 6 and 7.[20] Other non-linear editing systems indirectly support closed captioning only in Standard Definition line-21. Video files on the editing timeline must be composited with a line-21 VBI graphic layer known in the industry as a "blackmovie" with closed caption data.[21] Alternately, video editors working with the DV25 and DV50 firewire workflows must encode their DV .avi or .mov file with VAUX data which includes CEA-608 closed caption data. In the 1970s the Public Broadcasting Service contracted Texas Instruments to create a device that would allow the deaf to read what was being said on TV. Bill Kastner and his team designed a device that could decode voices and translate them into text. The first message ever to transmit over the airwaves was Float like a butterfly, sting like a bee. [22] Kastner did not recognize at the time how significant and popular closed captioning would become, and never applied for a patent. For that reason he doesn't refer to himself as the inventor. It was only late in his life that he publicly announced that he had designed closed captioning for the hearing impaired. [23] Regular open captioned broadcasts began on PBSs The French Chef in 1972.[24] WGBH began open captioning of ZOOM, ABC World News Tonight, and Once Upon a Classic shortly thereafter. Closed captioning was first demonstrated at the First National Conference on Television for the Hearing Impaired in Nashville, Tennessee in 1971.[24] A second demonstration of closed captioning was held at Gallaudet College (now Gallaudet University) on February 15, 1972 where ABC and the National Bureau of Standards demonstrated closed captions embedded within a normal broadcast of The Mod Squad. The closed captioning system was successfully encoded and broadcast in 1973 with the cooperation of PBS station WETA.[24] As a result of these tests, the FCC in 1976 set aside line 21 for the transmission of closed captions. PBS engineers then developed the caption editing consoles that would be used to caption prerecorded programs. Real-time captioning, a process for captioning live broadcasts, was developed in 1982.[24] In real-time captioning, court reporters trained to write at speeds of over 225 words per minute give viewers instantaneous access to live news, sports and entertainment. As a result, the viewer sees the captions within two to three seconds of the words being spoken. The National Captioning Institute was created in 1979 in order to get the cooperation of the commercial television networks.[25] The first use of regularly scheduled uses of closed captioning on American television occurred on March 16, 1980.[26] Sears had developed and sold the Telecaption adapter, a decoding unit that could be connected to a standard television set. The first programs seen with captioning were a Disney's Wonderful World presentation of the film Son of Flubber on NBC, an ABC Sunday Night Movie airing of Semi-Tough, and Masterpiece Theatre on PBS.[27] Until the passage of the Television Decoder Circuitry Act of 1990, television captioning was performed by a set-top box manufactured by Sanyo Electric and marketed by The National Captioning Institute (NCI). Through discussions with the manufacturer it was established that the appropriate circuitry integrated into the television set would be less expensive than the stand-alone box, and Ronald May, then a Sanyo employee, provided the expert witness testimony on behalf of Sanyo and Gallaudet University in support of the passage of the bill. On January 23, 1991, the Television Decoder Circuitry Act of 1990 was passed by US Congress.[24] This Act gave the Federal Communications Commission (FCC) power to enact rules on the implementation of Closed Captioning. This Act required all analog television receivers with screens of at least 13inches or greater, either sold or manufactured, to have the ability to display closed captioning by July 1, 1993.[28] Also in 1990, the Americans with Disabilities Act (ADA) was passed to ensure equal opportunity for persons with disabilities.[25] The ADA prohibits discrimination against persons with disabilities in public accommodations or commercial facilities. Title III of the ADA requires that public facilities, such as hospitals, bars, shopping centers and museums (but not movie theaters), provide access to verbal information on televisions, films or slide shows. The Telecommunications Act of 1996 expanded on the Decoder Circuity Act to place the same requirements on digital television receivers by July 1, 2002.[29] All TV programming distributors in the U.S. are required to provide closed caption for Spanish language video programming as of January 1, 2010.[30] A bill, H.R. 3101, the Twenty-First Century Communications and Video Accessibility Act of 2010, was passed by the United States House of Representatives in July 2010, and was signed by President Barack Obama on October 8, 2010. The Act requires, in part, for HDTV-decoding set-top box remotes to have a button to turn on or off the closed captioning in the output signal. It also requires broadcasters to provide captioning for television programs redistributed on the web.[31] The government of Australia provided seed funding in 1981 for the establishment of the Australian Caption Centre (ACC) and the purchase of equipment. Captioning by the ACC commenced in 1982 and a further grant from the Australian government enabled the ACC to achieve and maintain financial self-sufficiency. The ACC, now known as Media Access Australia, sold its commercial captioning division to Red Bee Media in December 2005. Red Bee Media continues to provide captioning services to Australia today.[32][33][34] The current and most familiar logo for closed captioning consists of two Cs (for "closed captioned") inside a television screen. It was created by Jack Foley while he was a senior graphic designer at WGBH.[citation needed] The other logo, trademarked by the National Captioning Institute, is that of a simple geometric rendering of a television set merged with the tail of a speech balloon; two such versions exist: one with a tail on the left, the other with a tail on the right.[35]
I read the paragraph on http://wikipedia.org
